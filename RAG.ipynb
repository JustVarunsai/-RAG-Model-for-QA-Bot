{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is done in google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai==0.28.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "GY4SHhGDmE6O",
        "outputId": "ab264a7a-190e-4342-fa43-a16de683f060"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "from functools import lru_cache\n",
        "\n",
        "# Configure logging for detailed error messages\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Pinecone and OpenAI API keys\n",
        "PINECONE_API_KEY = 'add yours'\n",
        "PINECONE_ENVIRONMENT = 'add env'\n",
        "OPENAI_API_KEY = 'add api key'  # Use your OpenAI key here\n",
        "\n",
        "# Initialize the Pinecone client with the API key\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# Initialize the index name\n",
        "index_name = 'rag'\n",
        "\n",
        "# Check if the index already exists, otherwise create a new one\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric='euclidean',\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region=PINECONE_ENVIRONMENT\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Connect to the existing or newly created index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Set OpenAI API key\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Example business documents\n",
        "docs = [\n",
        "    {\"id\": \"doc1\", \"text\": \"Our business operates from 9 AM to 5 PM.\"},\n",
        "    {\"id\": \"doc2\", \"text\": \"We offer free shipping for orders over $50.\"},\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to convert documents to embeddings using OpenAI's embeddings model with retries and jitter\n",
        "def get_embeddings_with_retry(text, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = openai.Embedding.create(input=[text], model=\"text-embedding-ada-002\")\n",
        "            return response['data'][0]['embedding']\n",
        "        except openai.error.RateLimitError:\n",
        "            wait_time = 2 ** attempt + random.uniform(0, 1)  # Add jitter to avoid rate-limit bottleneck\n",
        "            logging.warning(f\"Rate limit exceeded. Retrying in {wait_time:.2f} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    raise Exception(\"Failed to get embeddings after several attempts.\")\n",
        "\n",
        "# Cache embeddings for frequently queried documents to reduce API calls\n",
        "@lru_cache(maxsize=10)\n",
        "def get_cached_embeddings(doc_id, text):\n",
        "    return get_embeddings_with_retry(text)\n",
        "\n",
        "# Insert documents into Pinecone (consider batching for large datasets)\n",
        "batch_size = 100  # Adjust batch size as needed\n",
        "for i in range(0, len(docs), batch_size):\n",
        "    batch = docs[i:i + batch_size]\n",
        "    embeddings = []\n",
        "    for doc in batch:\n",
        "        try:\n",
        "            embedding = get_cached_embeddings(doc[\"id\"], doc[\"text\"])\n",
        "            if embedding and len(embedding) == 1536:\n",
        "                embeddings.append((str(doc[\"id\"]), embedding))\n",
        "            else:\n",
        "                raise ValueError(f\"Embedding size for {doc['id']} is incorrect or not available.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing document {doc['id']}: {e}\")\n",
        "\n",
        "    # Only attempt to upsert if embeddings are valid and non-empty\n",
        "    if embeddings:\n",
        "        try:\n",
        "            index.upsert(embeddings)\n",
        "            logging.info(f\"Successfully inserted batch {i // batch_size + 1}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error during upsert: {e}\")\n",
        "\n",
        "print(\"Documents inserted into Pinecone!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to retrieve relevant documents from Pinecone\n",
        "def retrieve_from_pinecone(query):\n",
        "    # Get query embeddings\n",
        "    query_embedding = get_embeddings_with_retry(query)\n",
        "\n",
        "    # Query Pinecone for relevant documents\n",
        "    results = index.query([query_embedding], top_k=2)\n",
        "\n",
        "    # Extract and return document IDs\n",
        "    matches = results['matches']\n",
        "    docs = [match['id'] for match in matches]\n",
        "\n",
        "    return docs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to generate an answer using GPT-4 and retrieved documents\n",
        "def generate_answer(docs, query):\n",
        "    # Combine documents and query\n",
        "    context = \"\\n\".join([f\"Document: {doc}\" for doc in docs])\n",
        "\n",
        "    # GPT-4 API call for answer generation\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=f\"{context}\\n\\nQuestion: {query}\\nAnswer:\",\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['text']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main RAG-based Q&A bot\n",
        "def rag_qa_bot(query):\n",
        "    # Step 1: Retrieve relevant documents from Pinecone\n",
        "    docs = retrieve_from_pinecone(query)\n",
        "\n",
        "    # Step 2: Generate an answer using GPT-4\n",
        "    answer = generate_answer(docs, query)\n",
        "\n",
        "    return answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example query\n",
        "query = \"What are your business hours?\"\n",
        "answer = rag_qa_bot(query)\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
