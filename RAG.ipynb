{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lPgt2cPnlXRq",
        "outputId": "1d8e24f0-f246-4a4e-e90a-d1c8c01c3817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28.0\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28.0) (4.12.2)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "GY4SHhGDmE6O",
        "outputId": "ab264a7a-190e-4342-fa43-a16de683f060"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Rate limit exceeded. Retrying in 1.46 seconds...\n",
            "WARNING:root:Rate limit exceeded. Retrying in 2.17 seconds...\n",
            "WARNING:root:Rate limit exceeded. Retrying in 4.11 seconds...\n",
            "ERROR:root:Error processing document doc1: Failed to get embeddings after several attempts.\n",
            "WARNING:root:Rate limit exceeded. Retrying in 1.51 seconds...\n",
            "WARNING:root:Rate limit exceeded. Retrying in 2.33 seconds...\n",
            "WARNING:root:Rate limit exceeded. Retrying in 4.18 seconds...\n",
            "ERROR:root:Error processing document doc2: Failed to get embeddings after several attempts.\n",
            "WARNING:root:Rate limit exceeded. Retrying in 1.29 seconds...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents inserted into Pinecone!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Rate limit exceeded. Retrying in 2.45 seconds...\n",
            "WARNING:root:Rate limit exceeded. Retrying in 4.85 seconds...\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "Failed to get embeddings after several attempts.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6a3e7c4f3235>\u001b[0m in \u001b[0;36m<cell line: 128>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m# Example query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What are your business hours?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_qa_bot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6a3e7c4f3235>\u001b[0m in \u001b[0;36mrag_qa_bot\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrag_qa_bot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m# Step 1: Retrieve relevant documents from Pinecone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_from_pinecone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# Step 2: Generate an answer using GPT-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6a3e7c4f3235>\u001b[0m in \u001b[0;36mretrieve_from_pinecone\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_from_pinecone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Get query embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Query Pinecone for relevant documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6a3e7c4f3235>\u001b[0m in \u001b[0;36mget_embeddings_with_retry\u001b[0;34m(text, retries)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Rate limit exceeded. Retrying in {wait_time:.2f} seconds...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to get embeddings after several attempts.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Cache embeddings for frequently queried documents to reduce API calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Failed to get embeddings after several attempts."
          ]
        }
      ],
      "source": [
        "import openai\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "from functools import lru_cache\n",
        "\n",
        "# Configure logging for detailed error messages\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Pinecone and OpenAI API keys\n",
        "PINECONE_API_KEY = 'add yours'\n",
        "PINECONE_ENVIRONMENT = 'add env'\n",
        "OPENAI_API_KEY = 'add api key'  # Use your OpenAI key here\n",
        "\n",
        "# Initialize the Pinecone client with the API key\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# Initialize the index name\n",
        "index_name = 'rag'\n",
        "\n",
        "# Check if the index already exists, otherwise create a new one\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric='euclidean',\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region=PINECONE_ENVIRONMENT\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Connect to the existing or newly created index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Set OpenAI API key\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Example business documents\n",
        "docs = [\n",
        "    {\"id\": \"doc1\", \"text\": \"Our business operates from 9 AM to 5 PM.\"},\n",
        "    {\"id\": \"doc2\", \"text\": \"We offer free shipping for orders over $50.\"},\n",
        "]\n",
        "\n",
        "# Function to convert documents to embeddings using OpenAI's embeddings model with retries and jitter\n",
        "def get_embeddings_with_retry(text, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = openai.Embedding.create(input=[text], model=\"text-embedding-ada-002\")\n",
        "            return response['data'][0]['embedding']\n",
        "        except openai.error.RateLimitError:\n",
        "            wait_time = 2 ** attempt + random.uniform(0, 1)  # Add jitter to avoid rate-limit bottleneck\n",
        "            logging.warning(f\"Rate limit exceeded. Retrying in {wait_time:.2f} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    raise Exception(\"Failed to get embeddings after several attempts.\")\n",
        "\n",
        "# Cache embeddings for frequently queried documents to reduce API calls\n",
        "@lru_cache(maxsize=10)\n",
        "def get_cached_embeddings(doc_id, text):\n",
        "    return get_embeddings_with_retry(text)\n",
        "\n",
        "# Insert documents into Pinecone (consider batching for large datasets)\n",
        "batch_size = 100  # Adjust batch size as needed\n",
        "for i in range(0, len(docs), batch_size):\n",
        "    batch = docs[i:i + batch_size]\n",
        "    embeddings = []\n",
        "    for doc in batch:\n",
        "        try:\n",
        "            embedding = get_cached_embeddings(doc[\"id\"], doc[\"text\"])\n",
        "            if embedding and len(embedding) == 1536:\n",
        "                embeddings.append((str(doc[\"id\"]), embedding))\n",
        "            else:\n",
        "                raise ValueError(f\"Embedding size for {doc['id']} is incorrect or not available.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing document {doc['id']}: {e}\")\n",
        "\n",
        "    # Only attempt to upsert if embeddings are valid and non-empty\n",
        "    if embeddings:\n",
        "        try:\n",
        "            index.upsert(embeddings)\n",
        "            logging.info(f\"Successfully inserted batch {i // batch_size + 1}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error during upsert: {e}\")\n",
        "\n",
        "print(\"Documents inserted into Pinecone!\")\n",
        "\n",
        "# Function to retrieve relevant documents from Pinecone\n",
        "def retrieve_from_pinecone(query):\n",
        "    # Get query embeddings\n",
        "    query_embedding = get_embeddings_with_retry(query)\n",
        "\n",
        "    # Query Pinecone for relevant documents\n",
        "    results = index.query([query_embedding], top_k=2)\n",
        "\n",
        "    # Extract and return document IDs\n",
        "    matches = results['matches']\n",
        "    docs = [match['id'] for match in matches]\n",
        "\n",
        "    return docs\n",
        "\n",
        "# Function to generate an answer using GPT-4 and retrieved documents\n",
        "def generate_answer(docs, query):\n",
        "    # Combine documents and query\n",
        "    context = \"\\n\".join([f\"Document: {doc}\" for doc in docs])\n",
        "\n",
        "    # GPT-4 API call for answer generation\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=f\"{context}\\n\\nQuestion: {query}\\nAnswer:\",\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['text']\n",
        "\n",
        "# Main RAG-based Q&A bot\n",
        "def rag_qa_bot(query):\n",
        "    # Step 1: Retrieve relevant documents from Pinecone\n",
        "    docs = retrieve_from_pinecone(query)\n",
        "\n",
        "    # Step 2: Generate an answer using GPT-4\n",
        "    answer = generate_answer(docs, query)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example query\n",
        "query = \"What are your business hours?\"\n",
        "answer = rag_qa_bot(query)\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
